---
title: Reference Architecture for Pivotal Cloud Foundry on Azure
owner: Customer0
---

<strong><%= modified_date %></strong>

This guide presents a reference architecture for Pivotal Cloud Foundry (PCF) on Azure.

Azure does not provide resources in a way that translates directly to PCF availability zones. Instead, Azure provides high availability through fault domains and [availability sets](https://docs.microsoft.com/en-us/azure/virtual-machines/virtual-machines-windows-manage-availability).

All reference architectures described in this topic are validated for production-grade PCF deployments using fault domains and availability sets that include multiple job instances. 

## <a id="arch_diagram"></a> Base Reference Architecture 

The following diagram provides an overview of a reference architecture deployment of PCF on Azure.

<%= image_tag('azure-overview-arch.png') %>

[View a larger version of this diagram](https://raw.githubusercontent.com/pivotal-cf/docs-pcf-refarch/master/images/azure-overview-arch.png).

### <a id="base_components"></a> Base Reference Architecture Components

The following table lists the components that are part of a base reference architecture deployment on Azure using a single resource group. 

<table class="nice">  
<th><strong>Component</strong></th>
<th><strong>Reference Architecture Notes</strong></th>
<tr>
 <td><strong>Domains & DNS</strong></td>  
 <td>CF Domain Zones and routes in use by the reference architecture include:
   <ul>
     <li>domains for *.apps and *.system (required),</li>
     <li>a route for Ops Manager (required),</li>
     <li>a route for Doppler (required),</li>
     <li>a route for Loggregator (required),</li>
     <li>a route for ssh access to app containers (optional),</li>
     <li>and a route for TCP routing to apps (optional). </li>          
   </ul>
 </td>
</tr>
<tr>
 <td><strong>Ops Manager</strong></td>  
 <td>Deployed on the infrastructure subnet and accessible by FQDN or through an optional Jumpbox.</td>
</tr> 
<tr>
 <td><strong>BOSH</strong></td>  
 <td>Deployed on the infrastructure subnet.</td>
</tr> 
<tr>
 <td><strong>Azure Load Balancer - API & Apps</strong></td>  
 <td>Required. Load balancer that handles incoming API and apps requests and forwards them to the Gorouter(s).</td>
</tr> 
<tr>
 <td><strong>Azure Load Balancer - ssh-proxy</strong></td>  
 <td>Optional. Load balancer that provides SSH access to app containers.</td>
</tr> 
<tr>
 <td><strong>Azure Load Balancer - tcp-router</strong></td>  
 <td>Optional. Load balancer that handles TCP routing requests for apps.</td>
</tr> 
<tr>
 <td><strong>Azure Load Balancer - MySQL</strong></td>  
 <td>Required to provide high availability for MySQL backend to Elastic Runtime.</td>
</tr> 
<tr>
 <td><strong>Gorouter(s)</strong></td>  
 <td>Accessed through the API & Apps load balancer. Deployed on the ERT subnet, one job per Azure availability set. </td>
</tr> 
<tr>
 <td><strong>Diego Brain(s)</strong></td>  
 <td>This component is required, however the SSH container access functionality is optional and enabled through the SSH Proxy load balancer. Deployed on the ERT subnet, one job per Azure availability set.</td>
</tr> 
<tr>
 <td><strong>TCP Router(s)</strong></td>  
 <td>Optional feature for TCP routing. Deployed on the ERT subnet, one job per availability zone.</td>
</tr> 
<tr>
 <td><strong>MySQL</strong></td>  
 <td>Reference architecture uses internal MySQL provided with PCF. Deployed on the ERT subnet, one job per Azure availability set.</td>
</tr> 
<tr>
 <td><strong>Elastic Runtime</strong></td>  
 <td>Required. Deployed on the ERT subnet, one job per Azure availability set.</td>
</tr> 
<tr>
 <td><strong>Storage Accounts</strong></td>  
 <td>PCF on Azure requires 5 standard storage accounts - BOSH, Ops Manager, and three ERT storage accounts. Each account comes with a set amount of disk. Reference architecture recommends using 5 storage accounts because Azure Storage Accounts have an IOPs limit (~20k, per each account), which generally relates to a BOSH JOB/VM limit of ~20 VMs each.</td>
</tr> 
<tr>
 <td><strong>Service Tiles</strong></td>  
 <td>Deployed on the PCF managed services subnet. Each service tile is deployed to an availability set.</td>
</tr> 
<tr>
 <td><strong>Dynamic Services</strong></td>  
 <td>Reserved for future use, dynamic services are deployed on their own subnet. Dynamic services are services autoprovisioned by BOSH based on a trigger, such as a request for that service. Pivotal recommends provisioning the multi-tenant dynamic services subnet as a /22 block.</td>
</tr>  
</table>

## <a id="common_network"></a> Alternative Network Layouts for Azure ##

This section describes the possible network layouts for PCF deployments as covered by the reference architecture of PCF on Azure. 

At a high level, there are currently two possible ways of deploying PCF as described by the reference architecture:

1. Single resource group, or
1. Multiple resource groups.

The first scenario is currently outlined in the [existing installation documentation](../../customizing/azure.html) for Azure deployments of PCF. It models a single PCF deployment in a single Azure Resource Group.

If you require multiple resource groups, you may refer to the [Multiple Resource Group deployment](#multiple_resource_groups) section.

### <a id="base-network-layout"></a> Network Layout

This diagram illustrates the network topology of the base reference architecture for PCF on Azure. In this deployment, you expose only a minimal number of public IP addresses and deploy only one resource group.

<%= image_tag('azure-net-topology-base.png') %>

[View a larger version of this diagram](https://raw.githubusercontent.com/pivotal-cf/docs-pcf-refarch/master/images/azure-net-topology-base.png).

### <a id="network_components"></a> Network Objects

The following table lists the network objects in PCF on Azure reference architecture.

<table class="nice">
  <th><strong>Network Object</strong></th>
  <th><strong>Notes</strong></th>
  <th><strong>Estimated Number</strong></th>
  <tr>
  <td><strong>External Public IP addresses</strong></td>
  <td>Use
   <ol>
     <li>global IP address for apps and system access</li>
     <li>Ops Manager (or optional Jumpbox).</li>
   </ol>
   Optionally, you can use a public IP address for the ssh-proxy and tcp-router load balancers.
  </td>
  <td>1-4</td>
  </tr>
  <tr>
  <td><strong>Virtual Network</strong></td>
  <td>One per deployment. Azure virtual network objects allow multiple subnets with multiple CIDRs, so a typical deployment of PCF will likely only ever require one Azure Virtual Network object.</td>
  <td>1</td>
  </tr>
  <tr>
  <td><strong>Subnets</strong></td>
  <td>Separate subnets for
   <ol>
     <li>infrastructure (Ops Manager, Ops Manager Director, Jumpbox),</li>
     <li>ERT,</li>
     <li>services,</li>
     <li>and dynamic services.</li>
   </ol>
   Using separate subnets allows you to configure different firewall rules due to your needs.
   </td>
  <td>4</td>
  </tr>
  <tr>
  <td><strong>Routes</strong></td>
  <td>Routes are typically created by Azure dynamically when subnets are created, but you may need to create additional routes to force outbound communication to dedicated SNAT nodes. These objects are required to deploy PCF without public IP addresses.</td>
  <td>3+</td>
  </tr>
  <tr>
  <td><strong>Firewall Rules</strong></td>
  <td>Azure firewall rules are collected into a Network Security Group (NSG) and bound to a Virtual Network object and can be created to use IP ranges, subnets, or instance tags to match for source and destination fields in a rule. One NSG can be used for all firewall rules.</td>
  <td>12</td>
  </tr>
  <tr>
  <td><strong>Load Balancers</strong></td>
  <td>Used to handle requests to Gorouters and infrastructure components. Azure uses 1 or more load balancers. The API and Apps load balancer is required. The TCP Router load balancer used for TCP routing feature and the SSH load balancer that allows SSH access to Diego apps are both optional. In addition, you can use a MySQL load balancer to provide high availability to MySQL. This is also optional.</td>
  <td>1-4</td>
  </tr>
  <tr>
  <td><strong>Jumpbox</strong></td>
  <td>Optional. Provides a way of accessing different network components. For example, you can configure it with your own permissions and then set it up to access to Pivotal Network to download tiles. Using a Jumpbox is particularly useful in IaaSes where Ops Manager does not have a public IP address. In these cases, you can SSH into Ops Manager or any other component through the Jumpbox. 
</td>
  <td>1</td>
  </tr>
</table>

### <a id="multiple_resource_groups"></a> Multiple Resource Group Deployment

This diagram illustrates the case where you want to use additional resource groups in your PCF deployment on Azure.

Shared network resources may already exist in an Azure subscription. In this type of deployment, using multiple resource groups allows you to reuse existing resources instead of provisioning new ones. 

To use multiple resource groups, you need to provide the BOSH Service Principal with access to the existing network resources. 

<%= image_tag('azure-multi-resgroup.png') %>

[View a larger version of this diagram](https://raw.githubusercontent.com/pivotal-cf/docs-pcf-refarch/master/images/azure-multi-resgroup.png).

###<a id="multi-resgroup-notes"></a> Multiple Resource Groups Deployment Notes

To deploy PCF on Azure with multiple resource groups, you can define custom roles to grant resource group access to your BOSH Service Principal. For example, you might develop the following:

* Dedicated `Network` Resource Group, limits BOSH Service Principal so that it does not have admin access to network objects.

* Custom Role for BOSH Service Principal, applied to `Network` Resource Group, limits the BOSH Service Principal to minimum read-only access.
  <pre class="terminal">
  az role definition create --role-definition
  { \
  "Name": "PCF Network Read Only", \
  "IsCustom": true, \
  "Description": "MVP PCF Read Network Resgroup", \
  "Actions": [ \
  "Microsoft.Network/virtualNetworks/read", \
  "Microsoft.Network/virtualNetworks/subnets/read", \
  "Microsoft.Network/virtualNetworks/subnets/join/action", \
  "Microsoft.Network/networkSecurityGroups/read", \
  "Microsoft.Network/networkSecurityGroups/join/action", \
  "Microsoft.Network/loadBalancers/read", \
  "Microsoft.Network/publicIPAddresses/read", \
  "Microsoft.Network/publicIPAddresses/join/action" \
  ], \
  "NotActions": [], \
  "AssignableScopes": ["/subscriptions/[YOUR\_SUBSCRIPTION\_ID]"] \
  } 
  </pre>
  The actions prefixed with `Microsoft.Network/publicIPAddresses` are only required if using IP addresses.

* Custom Role for BOSH Service Principal, applied to Subscription, allowing the Operator to deploy PCF components
  <pre class="terminal">
  az role definition create --role-definition
  { \
   "Name": "PCF Deploy Min Perms", \
   "IsCustom": true, \
   "Description": "MVP PCF Terraform Perms", \
   "Actions": [ \
     "Microsoft.Compute/register/action" \
   ], \
   "NotActions": [], \
   "AssignableScopes": ["/subscriptions/[YOUR\_SUBSCRIPTION\_ID]"] \
  }
  </pre>

* Custom roles can be assigned to service principals with `az role assignment create --role [ROLE] --assignee [SERVICE_PRINCIPAL_ID]`.

### <a id="multiple_resource_groups_multiple_load_balancer"></a> Multiple Resource Group and Multiple Load Balancer Deployment

This diagram illustrates the case where you want to deploy multiple load balancers with your multiple resource group deployment.

The key design points of this deployment are the following:

* Two Azure load balancer (ALBs) to the Gorouter exist. The first ALB is for API access, which utilizes a private IP address. The system domain should resolve to this ALB. The second ALB is for application access, which can either use a public or private IP address. The apps domain should resolve to this ALB.


### <a id="log_analytics_integration"></a>Log Analytics Integration

<a href="https://docs.microsoft.com/en-us/azure/log-analytics/log-analytics-overview">Azure Log Analytics</a> is a service that helps you collect and analyze data generated by resources in your cloud and on-premises environments. The <a href="http://docs.pivotal.io/partners/azure-log-analytics-nozzle">Microsoft Azure Log Analytics Nozzle for PCF</a> receives logs and metrics from the Loggregator Firehose, filters and resolves the events, and then sends them to Log Analytics, where they appear on unified dashboards, can be correlated with other Azure resources, and alerted on.


## DNS Delegation

Azure DNS supports DNS delegation, allowing for sub-level domains to be hosted within Azure. This functionality is fully supported within PCF. 

It is recommended to use a sub-zone for your PCF. For example, if your company's domain is `example.com`, your PCF zone in Azure DNS would be `pcf.example.com`. 

As Azure DNS does not support recursion, in order to properly configure Azure DNS, create an `NS` record with your registrar which points to the four name servers supplied by your Azure DNS Zone configuration. Once your `NS` records have been created, you can then create the required wildcard `A` records for the PCF application and system domains, as well as any other records desired for your PCF deployments. 

You do not need to make any configuration changes in PCF to support Azure DNS.
